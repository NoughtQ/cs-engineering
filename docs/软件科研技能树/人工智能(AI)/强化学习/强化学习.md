---
title: 强化学习
slug: ruan-jian-ke-yan-ji-neng-shu/ren-gong-zhi-neng-ai/qiang-hua-xue-xi/qiang-hua-xue-xi
sidebar_position: 6
---

# 强化学习

Author：潘驰昊

# Reinforcement Learning

- 智能体在一个环境中通过不断的交互以及<b>试错（trial-and-error）</b>来学习最佳行为策略

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>❗</div>
<p><b>找到一个策略，使智能体能够最大化从环境中获取的总奖励</b></p>
</div>

## <b>探索和利用</b>

- 探索：智能体尝试新的或者不确定的动作以收集更多信息
- 利用：智能体使用已有的知识选择看似最优的动作
- 在训练初期，智能体可能需要进行大量的探索以了解环境，随着学习的进行，智能体会逐渐转向利用
- 可以通过贪婪策略（Greedy Policy）或ε-贪婪策略（Epsilon-Greedy Policy）等方法找到平衡

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>❗</div>
<p><b>平衡探索和利用（Exploration and Exploitation）</b></p>
</div>

<img src="/assets/AcrTbw64YoLmD4xduL9c1rd6nyV.png" src-width="872" src-height="327"/>

## 关键组件概念

- <b>环境（Environment）</b>：环境包含了智能体可以交互的所有可能的状态和动作，智能体在环境中执行动作，并通过环境的反馈进行学习

- <b>智能体（Agent）</b>：智能体是一个可以从环境中学习的实体。它执行动作，观察环境的反馈，并更新其策略以改善未来的性能

- <b>状态（State）</b>：状态是环境的一种特定配置，可以包括各种信息，比如智能体的位置、速度、目标等。在每个状态下，智能体都需要做出决策，选择执行哪个动作（需要对状态进行编码）

- <b>动作（Action）</b>：动作是智能体在给定状态下可以执行的操作。选择哪个动作会影响智能体的下一个状态和从环境中获取的奖励

- <b>奖励（Reward）</b>：奖励是环境给予智能体的反馈，表示智能体最近执行的动作的好坏。强化学习的目标是通过学习找到一个策略，使得从环境中获取的总奖励最大化

- <b>策略（Policy）</b>：定义了智能体在给定状态下应该选择哪个动作
    <div class="callout callout-bg-2 callout-border-2">
    <div class='callout-emoji'></div>
    <p><b>智能体的策略通常被定义为关于其动作空间的概率分布</b></p>
    </div>
    1. 确定性策略：为每个状态选择一个特定的操作
    2. 随机策略：为每个操作分配一个概率
    
- <b>价值函数（Value Function）</b>：价值函数预测了在特定策略下，从当前状态或状态-动作对开始，预期能够获得的未来总奖励
    1. 状态价值函数（V值）：评估状态的价值，代表智能体在这个状态下，一直到最终状态的奖励总和的期望
    2. 动作价值函数（Q值）：评估动作的价值，代表智能体选择这个动作后，一直到最终状态奖励总和的期望
    <div class="callout callout-bg-2 callout-border-2">
    <div class='callout-emoji'>🎁</div>
    <p>一个状态的V值，就是这个状态下的所有动作的Q值，在策略Π下的期望</p>
    </div>

<div class="flex gap-3 columns-2" column-size="2">
<div class="w-[47%]" width-ratio="47">
<img src="/assets/R993bxMSEonwnRxwpOIc5HFtntf.png" src-width="807" src-height="331" align="center"/>
</div>
<div class="w-[52%]" width-ratio="52">
<img src="/assets/M3l0bVi7KoS1DVx39tccw5qHneg.png" src-width="1012" src-height="381" align="center"/>
</div>
</div>

- <b>马尔科夫决策过程（Markov Decision Process）</b>：马尔科夫决策过程是强化学习的数学模型，描述了状态、动作、转移概率和奖励的关系

## 简单训练过程

1. 智能体观察当前环境的状态
2. 根据其策略选择一个行动
3. 执行这个行动并接收环境的反馈，通常以奖励（reward）的形式表示
4. 智能体更新其策略以优化未来的奖励
5. 整个过程会反复进行，直到智能体的行为或性能满足某些预定的标准

## on-policy & off-policy

- <b>在线策略（on-policy）算法</b>：更新Q值时所使用的方法是沿用既定的策略，<b>直接学习和改进它正在使用的策略</b>（Sarsa和Actor-Critic等）

- <b>离线策略（0ff-policy）算法</b>：使用新策略更新Q值，一大优点是<b>可以使用历史数据</b>，并且可以学习一个最优策略，而不受到当前执行策略的限制（Q-learning和Deep Q-Network (DQN)等）

- <b>离线强化学习（offline reinforcement learning）：</b>在智能体不和环境交互的情况下，仅从已经收集好的确定的数据集中，通过强化学习算法得到比较好的策略

