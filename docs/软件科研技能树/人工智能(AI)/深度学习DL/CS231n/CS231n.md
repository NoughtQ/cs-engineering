---
title: CS231n
slug: ruan-jian-ke-yan-ji-neng-shu/ren-gong-zhi-neng-ai/shen-du-xue-xi-dl/cs231n/cs231n
sidebar_position: 0
---

# CS231n

Author：欧阳创宇

负责人：

# 线性回归

<em>回归</em>是能为一个或多个自变量与因变量之间关系建模的一类方法。<em>线性回归</em>可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量 $\textbf{x}$ 和因变量 $y$ 之间的关系是线性的， 即 $y$ 可以表示为 $\textbf{x}$ 中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

## 线性模型

线性回归的一个例子是对房价的预测。 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为<em>训练数据集</em> 或<em>训练集</em>。 每行数据（比如一次房屋交易相对应的数据）称为<em>样本</em>， 也可以称为<em>数据点</em>或<em>数据样本</em>。 我们把试图预测的目标（比如预测房屋价格）称为<em>标签</em>或<em>目标</em>。 预测所依据的自变量（面积和房龄）称为<em>特征</em>或<em>协变量</em>。

对于这个例子，我们可以假设如下的线性模型：

$$
price = \omega_{area} \cdot area + \omega_{age} \cdot age + b$$

其中 $\omega_{area}$ , $\omega_{age}$ 称为权重，$b$ 称为偏置。线性回归所做的，就是通过给定的数据集寻找合适的模型权重 $\omega$ 和 偏置 $b$ ，使得根据模型做出的预测大体符合数据里的真实价格。这样的模型是对输入特征的仿射变换，通过加权和来对特征进行线性变换，通过偏置来进行平移。

由于机器学习领域中通常使用的是高维的数据集，因此在模型的表示上用线性代数表示会比较方便。假设输入包含 $d$ 个特征，则预测结果 $\hat{y}$ 可表示为：

$$
\hat{y} = \textbf{w}^T\textbf{x} + b$$

其中 $\textbf{x} \in \mathbb{R}^d$ 为特征向量， $\textbf{w} \in \mathbb{R}^d$ 为权重向量。这个式子是对单个数据样本的模型，对于整个数据集，可以用矩阵 $\textbf{X} \in \mathbb{R}^{n \times d}$ 表示，其中 $\textbf{X}$ 的行数表示样本数量，列数表示特征维数。对于这样的样本矩阵 $\textbf{X}$ ，预测值 $\hat{\textbf{y}} \in \mathbb{R}^n$ 可以用如下式子表示：

$$
\hat{\textbf{y}} = \textbf{Xw} + b$$

## 损失函数

在寻找合适的模型参数 $\textbf{w}$ 和 $b$ 之前，我们需要先定义如何来衡量一组参数的优劣。<em>损失函数</em>（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为 0 。

在回归问题中很常用的一种损失函数是平方误差函数。当样本 $i$ 的预测值为 $\hat{y}^{(i)}$ ，其对应的真实值为 $y^{(i)}$ 时，平方误差可以定义为如下公示：

$$
l^{(i)}(\textbf{w}, b) = \frac12(\hat{y}^{(i)}-y^{(i)})^2$$

这里常数 $\frac12$ 主要是使得损失函数在求导后常数系数为 $1$ ，形式更美观。对于度量模型在整个数据集上的表现，还需要对每一个数据样本的损失值求和，即

$$
L(\textbf{w}, b) = \frac1n \sum_{i=1}^n l^{(i)}(\textbf{w}, b) = \frac1n \sum_{i=1}^n \frac12 (\textbf{w}^T\textbf{x}^{(i)} + b - y^{(i)}) ^2$$

有了参数质量的衡量方法，我们便可以明确模型训练的目标，即寻找一组参数 $(\textbf{w}^*, b^*)$ ，使得线性回归模型在所有训练样本上的总损失最小，也即下式：

$
\textbf{w}^*, b^* = \mathop{\text{argmin}} \limits_{\textbf{w}, b} \ L(\textbf{w}, b)$$
\textbf{w}^*, b^* = \mathop{\text{argmin}} \limits_{\textbf{w}, b} \ L(\textbf{w}, b)$$
\textbf{w}^*, b^* = \mathop{\text{argmin}} \limits_{\textbf{w}, b} \ L(\textbf{w}, b)$

## 解析解

线性回归模型的优化是一个很简单的优化模型，可以通过数学计算得到一个简洁的解析解。在矩阵 $\textbf{X}$ 中添加一列 $1$ ，将参数 $b$ 加到 $\textbf{w}$ 中，可以将模型目标转化为最小化 $||\textbf{y} - \textbf{Xw}||^2$ ，由此可得到解析解

$$
\textbf{w}^* = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y}$$

当然，解析解只有对这样简单的模型有效，并不是所有的问题都存在解析解。

## 随机梯度下降

在无法得到模型解析解的情况下，通过<em>梯度下降</em>的方式也可以有效地训练模型。这种方法的实现方式是不断地在损失函数递减的方向上更新参数来降低误差。为了降低计算的复杂度，在每次更新时往往会随即抽取一小批样本来代替整个数据集，这种变体叫做<em>小批量随机梯度下降</em> ，其具体执行步骤如下：

1. 初始化模型参数
2. 随机抽取一个小批量样本 $\mathcal{B}$
3. 计算该样本的平均损失关于模型参数的导数（称为梯度）
4. 将梯度乘以一个正数超参数 $\eta$ ，并从当前参数的值中减掉
5. 重复步骤 2 ~ 4 ，直到达到预先确定的迭代次数或满足某些停止条件

用数学公示来表示这一更新过程为：

$$
(\textbf{w}', b') = (\textbf{w}, b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\textbf{w}, b)}l^{(i)}(\textbf{w}, b)$$

对于仿射变换和平方损失，这一更新式可以明确写为如下形式：

$$
\begin{align} \textbf{w}' &= \textbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\textbf{w}} l^{(i)}(\textbf{w}, b) =  \textbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \textbf{x}^{(i)} (\textbf{w}^T\textbf{x}^{(i)} + b - y^{(i)}) \\ b' &= b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{b} l^{(i)}(\textbf{w}, b) =  b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}(\textbf{w}^T\textbf{x}^{(i)} + b - y^{(i)}) \end{align}$$

式中的 $|\mathcal{B}|$ 和 $\eta$ 分别表示随机抽取小批量样本 $\mathcal{B}$ 的样本数和学习率 $\eta$ ，它们不是通过训练得到的，而是在训练之前手动指定的，称为超参数。通常我们需要根据训练迭代的结果来调整超参数的选择。

## 正态分布与平方损失

如果考虑数据中存在噪声，且噪声服从正态分布，则模型可以做如下修改：

$$
y = \textbf{w}^T \textbf{x} + b + \epsilon$$

其中 $\epsilon \sim \mathcal{N}(0, \sigma^2)$ 。

由此可得由给定的 $\textbf{x}$ 得到特定结果 $y$ 的似然：

$$
P(y \mid \textbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp((-\frac{1}{2\sigma^2}(y-\textbf{w}^T \textbf{x} - b)^2)$$

此时可由极大似然估计法来确定使得整个数据集似然最大的参数 $\textbf{w}$ 和 $b$ 的值。对整个数据集，似然值为：

$$
P(\textbf{y} \mid \textbf{X}) = \prod_{i=1}^n p(y^{(i)} \mid \textbf{x}^{(i)})$$

对此函数求负对数（由于历史原因，优化通常是说最小化而不是最大化，所以取负），可得：

$$
-\ln P(\textbf{y} \mid \textbf{X}) = \sum_{i=1}^n \frac12 \ln(2 \pi \sigma^2) + \frac{1}{2 \sigma^2}(y^{(i)} - \textbf{w}^T \textbf{x}^{(i)} + b)^2$$

可见如果我们假设噪声方差 $\sigma^2$ 是固定常数，那么对含噪声模型的极大似然估计与之前分析的最小化均方误差是等价的。

## 神经网络图

线性回归模型实际上可以看作是单个的计算神经元，如下图。在这种表示中，图上只显示了连接方式，而隐去了权重和偏置的值。

<img src="/assets/UplRbgivMotFA0xQ47Ncm9t0nKe.png" src-width="394" src-height="138" align="center"/>

对于线性回归，每个输入都与每个输出相连，我们称这种变换为<em>全连接层</em>或<em>稠密层</em>。

# 线性回归的从零开始实现

## 导入相关库

```py
import random
import torch
from d2l import torch as d2l
```

## 生成数据集

在本次实践中，我们将根据带有噪声的线性模型来构造一个人造数据集，然后试图利用线性回归的方法通过这个数据集来恢复模型的参数。在下面的代码中，我们生成一个包含 1000 个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。

我们使用的线性模型参数为 $\textbf{w} = [2, -3.4]^T$ 、$b = 4.2$ 和噪声项 $\epsilon \sim \mathcal{N}(0, 0.01)$ 生成数据集及其标签：

$$
y = \textbf{Xw} + b + \epsilon$$

我们用如下代码进行数据集的生成：

```py
def synthetic_data(w, b, num_examples):
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

得到的 `features` 的每一行为一个二维的数据样本，`labels` 每一行为对应的标签值。

```py
print('features:', features[0],'\nlabel:', labels[0])
```

```text
features: tensor([ 0.7579, -0.4018]) 
label: tensor([7.0914])
```

通过生成第二个特征 `features[:, 1]` 和 `labels` 的散点图， 可以直观观察到两者之间的线性关系。

```py
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);
d2l.plt.show()
```

<img src="/assets/Uz5ybmAPdo0QZUxnFBLcWWVOnqb.png" src-width="699" src-height="501" align="center"/>

## 读取数据集

在线性回归模型训练的过程中，每次取出的是其中的一份小批量数据集用于更新参数。为了方便地取出小批量随即数据集，我们定义一个函数来做这件事。

```py
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

这个函数通过 `yield` 关键字成为了一个生成器，可以很方便地用在迭代中，如下：

```py
batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    # do something...
```

## 初始化模型参数

根据模型训练的流程，首先需要给一个初始的参数。这里我们通过正态分布得到一个随机的初始权重，并把初始偏置设为 0 。由于需要计算损失函数对参数的导数，故设 `requires_grad` 为 `True` 。

```py
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

## 定义模型

模型的定义也就是把模型的输入（数据集）和参数与输出关联起来，于是可以得到如下函数：

```py
def linreg(X, w, b):
    return torch.matmul(X, w) + b
```

## 定义损失函数

模型优化的一大核心便是损失函数对各参数的导，所以损失函数的定义也是必要的一步。在这里我们使用前文提到的平方损失函数。

```py
def squared_loss(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

## 定义优化算法

接下来就可以定义迭代过程中参数优化的算法了。这里采用的是前文讲述的小批量随机梯度下降更新。

```py
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

这里通过 `with torch.no_grad()` 将更新过程包裹起来，其目的是防止将更新参数的计算步骤也纳入计算图中影响反向传播。

## 训练

上述内容都定义好了之后，就可以正式把它们搭起来，实现主要的训练部分了。这里再把训练过程简要概括一下：

1. 初始化参数
2. 抽取样本
3. 计算梯度
4. 更新参数

在每个迭代周期中，我们通过 `data_iter` 函数来遍历整个数据集，对参数进行更新。这里的迭代次数 `num_epochs` 、小批量训练集大小 `batch_size` 和学习率 `lr` 都是超参数。

```py
lr = 0.03
num_epochs = 3
batch_size = 10
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

在上述超参数的设置下，模型训练得到的结果是：

```text
epoch 1, loss 0.037547
epoch 2, loss 0.000137
epoch 3, loss 0.000047
```

事实上由于真正的参数是已知的，所以我们可以直接对真实的参数和训练得到的参数进行比较。

```py
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```

结果如下：

```text
w的估计误差: tensor([-1.9073e-05, -5.3620e-04], grad_fn=<SubBackward0>)
b的估计误差: tensor([0.0003], grad_fn=<RsubBackward1>)
```

可以看到两者误差非常小。

## 完整代码

```py
import random
import torch
from d2l import torch as d2l

def synthetic_data(w, b, num_examples):
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]

def linreg(X, w, b):
    return torch.matmul(X, w) + b

def squared_loss(y_hat, y):
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)

w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

lr = 0.03
num_epochs = 3
batch_size = 10
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')

print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```

# 线性回归的简洁实现

简洁实现指利用深度学习框架封装好的 api 进行训练代码的编写。一个简单的例子如下，实现的仍然是第 2 部分的内容。

```py
import numpy as np
import torch
from torch import nn
from torch.utils import data
from d2l import torch as d2l

def load_array(data_arrays, batch_size, is_train=True):
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)
    # 将数据按照 batch_size 封装，便于后续迭代使用

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

net = nn.Sequential(nn.Linear(2, 1))
"""
Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层，
然后将第一层的输出作为第二层的输入，以此类推。
在本例中用到的层被称为全连接层（fully-connected layer）， 因为它的每一个输入都通过
矩阵-向量乘法得到它的每个输出。在PyTorch中，全连接层在Linear类中定义。 值得注意的是，
我们将两个参数传递到nn.Linear中。第一个指定输入特征形状，即2，第二个指定输出特征形状，
输出特征形状为单个标量，因此为1。
"""
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()
# 计算均方误差使用的是MSELoss类，也称为平方范数。 默认情况下，它返回所有样本损失的平均值。
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        # 清除上一次的梯度
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)
运行结果：
epoch 1, loss 0.000341
epoch 2, loss 0.000094
epoch 3, loss 0.000095
w的估计误差： tensor([0.0010, 0.0005])
b的估计误差： tensor([0.0004])
```

# softmax 回归

分类问题可以分为两类：1. 只关心样本属于哪一类别；2. 希望得到样本属于每一类别的概率。前者称为硬类别，后者称为软类别。通常情况下，即使我们只关心硬类别，在模型的构建中仍然会使用软类别。

## 分类问题

我们以图像分类问题为例来构建软分类问题的输入输出要素。假设每次输入的是一张 $2 \times 2$ 的灰度图像，则我们可以用一个标量来表示每一个像素值，那么每张图像对应四个特征值 $x_1, x_2, x_3, x_4$ 。于是输入转化成了和线性回归模型一样的形式。

对于输出，假设每张图像属于类别“猫”“鸡”和“狗”中的一个，那么一个很明显的想法是选择 $y \in \{1, 2, 3\}$ ，每个整数分别代表一个动物。这种标签对于有自然顺序关系的分类比较有意义，例如年龄段的分类 $\{\text{婴儿}, \text{儿童}, \text{青少年}, \text{青年人}, \text{中年人}, \text{老年人}\}$ 。但是对于我们这个例子，当最后得到的结果落在某两个标签之间时，它的意义并不明确。于是我们考虑另外一种标签：<em>独热编码</em>（one-hot encoding）。这种编码方式的输出结果是一个向量，它的维度和类别一样多，类别对应的分量是 $1$ ，其余分量是 $0$ 。在此例中，标签 $y$ 是一个三维向量，其中 $(1, 0, 0)$ 对应“猫”，$(0, 1, 0)$ 对应“鸡”，$(0, 0, 1)$ 对应狗，即：

$$
y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$

联系我们之前提到的软类别，这种编码方式实际也可以看作样本属于每一类的概率，于是模型计算得到的样本标签应该是一个各分量为非负数且和为 $1$ 的向量。         

至此，输入输出要素已经构建完毕。

## 网络架构

由 4.1 得到的输入输出可知，软分类问题实际需求就是根据输入的特征值来计算输出向量的值。这就回到了前文所述的回归问题，只不过这次的模型不再是一个输出，而是有多个输出。因此，我们需要构建和输出维度数量一致的仿射变换式，每一个式子对应一个类别的输出。在本例中，由于我们有 4 个特征和 3 个可能的输出类别， 我们将需要 12 个标量来表示权重（带下标的 $w$ ）， 3个标量来表示偏置（带下标的 $b$ ）。 下面我们为每个输入计算三个<em>未规范化的预测</em>（logit）：$o_1$、$o_2$ 和 $o_3$ 。

$$
\begin{align} o_1 &= x_1 w_{11} + x_2 w _{12} + x_3 w_{13} + x_4 w_{14} + b_1 \\ o_2 &= x_1 w_{21} + x_2 w _{22} + x_3 w_{23} + x_4 w_{24} + b_2 \\ o_3 &= x_1 w_{31} + x_2 w _{32} + x_3 w_{33} + x_4 w_{34} + b_3 \end{align}$$

我们可以和之前的线性回归一样用神经网络图来描述这个过程。这里的输出层也是全连接层。

<img src="/assets/UmHZbzv0QoNOqPx9Sy9cWEXinXg.png" src-width="394" src-height="138" align="center"/>

同样我们也可以用线性代数的表示方式来简化上述仿射变换式，即

$$
\textbf{o} = \textbf{Wx} + \textbf{b}$$

## softmax 运算

对于一个样本，我们希望模型的输出 $\hat{y_j}$ 可以视为此模型属于类 $j$ 的概率，然后选择具有最大输出值的类别 $\text{argmax}_j y_j$ 作为预测类别。现在模型优化的目标就是寻找合适的参数来最大化目标数据的概率值。然而仅由 4.2 中得到的模型输出 $o$ 并不能直接作为我们想得到的概率输出值，因为我们并没有对仿射变换式加以限制，导致输出向量各维度之和不一定是 $1$ ，同时各维度上的分量也可能出现负数。

为了解决这个问题我们可以对得到的 $\textbf{o}$ 应用 <em>softmax</em> 函数：

$$
\hat{\textbf{y}} = \text{softmax}(\textbf{o}) \quad \text{其中} \quad \hat{y_j} = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$

这个函数首先通过求幂将未规范化的预测限制在非负数集合内，然后再通过除以总和来使各维度分量之和为 $1$ ，因此，$\hat{\textbf{y}}$ 可以视为一个正确的概率分布。

尽管 softmax 是一个非线性函数，但是其输出仍然由输入特征的仿射变换决定，因此， softmax 回归是一个<em>线性模型</em>。

## 损失函数

### 对数似然

softmax 函数给出的向量 $\hat{\textbf{y}}$ 可以视为对给定输入 $\textbf{x}$ 的每个类的条件概率，例如还是对于上例，$\hat{y_1} = P(y = \text{猫} \mid \textbf{x})$ 。对于有 $n$ 个样本的数据集 $\{\textbf{X}, \textbf{Y}\}$ ，我们可以计算其似然值：

$$
P(\textbf{Y} \mid \textbf{X}) = \prod_{i=1}^n P(\textbf{y}^{(i)} \mid \textbf{x}^{(i)})$$

要最大似然估计，我们可以最小化似然的负对数：

$$
-\ln{P(\textbf{Y} \mid \textbf{X})} = \sum_{i=1}^n -\ln(P(\textbf{y}^{(i)} \mid \textbf{x}^{(i)})) = \sum_{i=1}^n l(\textbf{y}^{(i)}, \hat{\textbf{y}}^{(i)})$$

为了更直观地表示损失函数 $l(\textbf{y}, \hat{\textbf{y}})$ ，我们希望能用关于 $\textbf{y}$ 和 $\hat{\textbf{y}}$ 的式子来表示 $P(\textbf{y}^{(i)} \mid \textbf{x}^{(i)})$ 。考虑到 $\textbf{y}$ 是一个长度为 $q$ 的独热编码，其中除了表示其正确类别的分量为 $1$ 以外其余分量都是$0$ ，因此可以考虑用如下方式表示：

$$
P(\textbf{y}^{(i)} \mid \textbf{x}^{(i)}) = \sum_{j=1}^q y_j \hat{y_j}$$

于是我们得到对任意一个样本的损失函数：

$$
l(\textbf{y}, \hat{\textbf{y}}) = -\sum_{j=1}^q y_j \ln(\hat{y_j})$$

当正确预测标签时，损失函数取到最小值 $0$ ，符合需求。

这个损失函数实际上称为<em>交叉熵损失</em>，它是分类问题最常用的损失之一。有关熵和交叉熵损失的内容将在 4.5 节中简单展开。

### softmax 及其导数

根据 $\text{softmax}$ 函数定义，可得

$$
\begin{align} l(\textbf{y}, \hat{\textbf{y}}) &= -\sum_{j=1}^q y_j \ln(\frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)}) \\  &= \sum_{j=1}^q y_j \ln{\sum_{k=1}^q \exp(o_k)} - \sum_{j=1}^q y_j o_j \\  &= \ln{\sum_{k=1}^q \exp(o_k)} - \sum_{j=1}^q y_j o_j \end{align}$$

将它对 $o_j$ 求导，得

$$\partial_{o_j} l(\textbf{y}, \hat{\textbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \text{softmax}(\textbf{o})_j - y_j$$

可以看到这一结构与线性回归中损失函数的导数非常相似。

## 信息论基础

### 信息量与熵

在信息论中，<em>熵</em>是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。值域为 $\{x_1, \dots , x_n\}$ 的随机变量 $X$ 熵的计算式如下，它表示的是对随机变量 $X$ 的信息量的期望值：

$$
H(X) = E[I(X)] = \sum_i P(x_i)I(x_i) = -\sum_i P(x_i) \log_b{P(x_i)}$$

在这里 $b$ 是对数所使用的底，通常是 $2$ ，自然常数 $e$ ，或是 $10$ 。当 $b = 2$ ，熵的单位是 bit ；当 $b = e$ ，熵的单位是 nat ；而当 $b = 10$ ，熵的单位是 Hart 。信息论的基本定理之一指出，为了对从分布为 $P$ 的随机变量 $X$ 中随机抽取的数据进行编码，我们至少需要 $H(X)$ 的消息长度对其进行编码。

在上式中， $I(x)$ 表示 $x$ 的信息量，其计算式为

$$
I(x) = -\log_b{P(x)}$$

我们以抛硬币为例。如果有一枚理想的硬币，其出现正面和反面的机会相等，我们无法预测下一次硬币抛掷的结果。在这种情况下，抛一次硬币的信息量就是 $-\log_b \frac12$ 。假设取 $b = 2$ ，则信息量为 $1$ ，熵为 $1$ ，也就是需要一比特的消息长度来对一次硬币的结果进行编码（$0$ 或 $1$）。而如果现在有另外一枚硬币，它的两面完全相同，则这种情况下抛一次硬币的信息量就是 $-\log_b 1 = 0$ ，熵也为 $0$ ，因为结果可以被准确预测，“下一次抛掷的结果为xxx”毫无信息。

### 相对熵

- [推荐资料：KL散度(Kullback-Leibler Divergence)介绍及详细公式推导](https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/)

<em>KL散度</em> ，又称<em>相对熵</em>，是两个概率分布 $P$ 和 $Q$ 差别的非对称性的度量，它是用来度量使用基于 $Q$ 的分布来编码服从 $P$ 的分布的样本所需的额外的平均比特数。典型情况下， $P$ 表示数据的真实分布， $Q$ 表示数据的理论分布、估计的模型分布、或 $P$ 的近似分布。而在机器学习中，或者更具体的，在本节所讲述的情况下， $P$ 用来表示样本的真实概率标签，即一个独热编码向量； $Q$ 用来表示模型所预测的概率向量。一个对这种情况下的相对熵直观的理解是，用 $Q$ 来描述样本，只能大致描述，但不是精确的，因此在信息量上比 $P$ 少，需要额外补充大小为相对熵 $D_{KL}(P||Q)$ 的信息增量才能准确描述。相对熵越小，则 $Q$ 的预测结果越好。

相对熵的计算公示为：

$$
D_{KL}(P||Q) = \sum_i P(x_i)\log_b{\frac{P(x_i)}{Q(x_i)}}$$

### 交叉熵

对相对熵的计算式变形可得：

$$
\begin{align} D_{KL}(P||Q) &= \sum_i P(x_i) \log_b{P(x_i)} - \sum_i P(x_i) \log_b{Q(x_i)} \\  &= -H(P) + [- \sum_i P(x_i) \log_b{Q(x_i)}] \end{align}$$

等号右边的第二项就是 $P$ 和 $Q$ 的交叉熵，用 $H(P, Q)$ 表示。上式移项后可得：

$$
H(P, Q) = H(P) + D_{KL}(P||Q)$$

即 $P$ 与 $Q$ 的交叉熵大小等于 $P$ 的熵加上 $P$ 与 $Q$ 的相对熵。在本例中，由于样本的概率分布是确定的，故根据交叉熵的大小即可衡量相对熵的大小。这就是交叉熵损失函数优化模型的原理。

综上，交叉熵损失函数可从两个角度理解：(1) 最大化似然 (2) 最小化相对熵。

# 图像分类数据集

机器学习离不开数据集的使用，本节将简单介绍通过 pytorch 下载和使用 Fashion-MNIST 数据集。

## 导入相关库

```py
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l
```

## 下载数据集

torchvision 库中内置了 Fashion-MNIST 数据集的下载函数 `torchvision.datasets.FashionMNIST()` ，可通过这个函数将数据集下载并读取到内存中。

```py
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

这里用到了 `torchvision.transforms.ToTensor()` 函数作为下载函数的 `transform` 参数，原因是下载样本的默认格式是 PIL 图片格式，这个函数将把图像数据从 PIL 变换成 $[0, 1]$ 之间的 32 位浮点数。

因为网络原因直接通过这个函数下载可能会非常非常慢，这种情况下可以访问 [zalandoresearch / fashion-mnist](https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion) 来手动下载四个数据包到指定目录，这样代码在运行的时候就会跳过下载直接解压。

## 数据集内容

Fashion-MNIST 由 10 个类别的图像组成， 每个类别由训练数据集中的 6000 张图像和测试数据集中的 1000 张图像组成。 因此，训练集和测试集分别包含 60000 和 10000 张图像。 测试数据集不会用于训练，只用于评估模型性能。每个输入图像均为 $28 \times 28$ 的灰度图像。

```py
print(mnist_train[0][0].shape)
```

```text
torch.Size([1, 28, 28])
```

Fashion-MNIST 中包含的 10 个类别，分别为 t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。

```py
def get_fashion_mnist_labels(labels):
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```

这里我们可以用一个函数来可视化这些图片数据。函数主要部分就是利用 `d2l.plt` 来实现数据信息到可视化图片的过程。

```py
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # 图片张量
            ax.imshow(img.numpy())
        else:A
            # PIL图片
            ax.imshow(img)
        # 隐藏座标轴
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes
```

我们可以试着显示训练集中的前 18 个样本及其对应的标签。

```py
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))
```

<img src="/assets/RZIub7xZZoPLgZxZyT6coOdCnyd.png" src-width="1350" src-height="300" align="center"/>

## 读取数据集

在第 3 节和第 5.3 节中我们都用到了 `data.DataLoader` 来实现数据迭代器。事实上，对这个类我们还可以通过 `num_workers` 参数指定读取数据所用的进程数。如下是一个典型的训练集读取方式：

```py
batch_size = 256

def get_dataloader_workers():
    return 4

train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
     num_workers=get_dataloader_workers())
```

## 整合所有组件

下面将把上述有关组件整合起来，展示一个简单的获取和读取 Fashion-MNIST 数据集的函数。

```py
def load_data_fashion_mnist(batch_size, resize=None):
    """下载Fashion-MNIST数据集，然后将其加载到内存中"""
    trans = [transforms.ToTensor()]
    if resize:
        # 图像大小缩放
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
    
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
```

```go
torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64
```

可以看到 $X$ 为 32 个单通道 $64 \times 64$ 的图片数据，$Y$ 为 $32$ 个标签数据。

# softmax 回归的从零开始实现

## 导入相关库

```py
import torch
from IPython import display
from d2l import torch as d2l
```

## 初始化数据集

在 5.5 节中用到的的 `load_data_fashion_mnist` 函数已经在 `d2l` 库中定义，我们可以直接调用该函数来实现 Fashion-MNIST 数据集的下载和初始化。

```py
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

## 初始化模型参数

在第 3 节线性回归的实现中，我们用一个一维向量来表示一个数据样本。对于本例中的图片样本也一样。原始数据集中的样本是 $28 \times 28$ 的灰度图，这里我们将它展平为一个长为 784 的一维向量。

考虑第 4 节中讲述的分类问题的方程。方程的个数与类别数相等，每个方程中都有数量等同于样本特征数的权重以及一个偏置。对于 Fashion-MNIST 数据集，类别数为 10 ，样本特征数为 784 ，故权重矩阵大小为 $784 \times 10$ ，偏置矩阵大小为 $1 \times 10$ 。与线性回归一样，我们将使用正态分布初始化我们的权重 $\textbf{W}$ ，偏置初始化为 0 。

```py
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
```

## 定义 softmax 操作

根据几个参数矩阵大小的规定，我们通过矩阵乘法得到的 $\textbf{o}$ 矩阵大小应该为 $\text{样本数} \times 10$ 。由此，实现 softmax 运算的步骤如下：

1. 对每一项求幂
2. 对每一行求和，得到每个样本的规范化常数
3. 对每一项除以其所在行对应的规范化常数

即如下表达式：

$$
\text{softmax}(\textbf{o})_{ij} = \frac{\exp(\textbf{o}_{ij})}{\sum_k \exp(\textbf{o}_{ik})}$$

于是可以定义如下 softmax 函数：

```py
def softmax(o):
    o_exp = torch.exp(o)
    partition = o_exp.sum(1, keepdim=True)
    return o_exp / partition
```

注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。 矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。这一部分的优化将在 7.1 中简单展开。

## 定义模型

在定义了参数及 softmax 函数后，我们就可以定义 softmax 回归的模型了，也就是如下关系：

$$
\hat{\textbf{y}} = \text{softmax}(\textbf{Xw} + \textbf{b})$$

```py
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

这里通过 `reshape` 函数将原始的图像数据展开为一维向量。

## 定义损失函数

在 4.4 和 4.5 两小节中我们介绍了交叉熵损失函数，这也是我们将实现的损失函数。值得一提的是，这里我们不使用 for 循环来遍历，而是通过 range 来实现所有元素的选择。

```py
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])
```

## 分类精度

对于模型输出的概率分布 $\hat{\textbf{y}}$ ，当我们需要对样本做出硬预测时，我们通常会选择概率最高的那一类。当预测与标签分类一致时，预测正确。分类精度即正确预测数量与总预测数量之比。精度通常是我们最关心的性能衡量标准，我们在训练分类器时几乎总会关注它。而不选择分类精度作为损失函数主要是由于精度的计算不可导，很难直接优化。

为了计算精度，我们执行以下操作。由于 softmax 操作不改变矩阵大小，因此精度计算函数的操作对象大小仍然是 $\text{样本数} \times 10$ ，即第二个维度存储每个类的预测概率。因此我们可以使用 `argmax` 来获得每行中最大元素的索引来预测其类别，然后使用 `==` 运算符来得到一个结果仅包含 0 / 1 的向量。对这个向量求和即可得到正确预测的数量。需要注意的是 `==` 运算符对类型敏感，因此我们要将 `y_hat` 的数据类型转化为与 `y` 一致。

```py
def accuracy(y_hat, y):
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

由于程序中会多次用到像记录正确预测个数这样的计数需求，我们定义一个类 `Accumulator` 用于计数。一个 `Accumulator` 实例中包含 n 个变量，分别表示一个我们需要进行计数的对象。

```py
class Accumulator:
    """在n个变量上累加"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, <em>args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
</em><em>        self.data = [0.0] </em> len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

利用 `accuracy` 函数和 `Accumulator` 对象，我们便可以实现对指定数据集的分类精度的计算。

```py
def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

## 训练

softmax 回归的训练函数和线性回归非常相似，这里不赘述，仅在关键部分加以注释。

```py
def train_epoch_ch3(net, train_iter, loss, updater):
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            # 内置优化器不包含除以 batch_size 部分，故损失函数的结果取均值
            updater.step()
        else:
            # 使用自定义的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
            # 自定义优化器含除以 batch_size ，故损失函数的结果取和
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

我们可以定义一个 `Animator` 类用于动态展现每一步优化的结果。

```py
class Animator:
    """在动画中绘制数据"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

有了单次训练函数和过程绘制类，我们便可以将它们组合成多轮次的训练函数。该训练函数将会运行多个迭代周期（由`num_epochs`指定）。 在每个迭代周期结束时，利用`test_iter`访问到的测试数据集对模型进行评估。

```py
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

对于优化器，我们仍然使用第 2 节所定义的 `sgd` 函数（该函数包含在 `d2l` 包内）。

```py
lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)
```

现在我们进行 10 轮的训练。

```py
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```

训练进度的可视化结果如下：

<img src="/assets/NopRbOy1EoCom5xrMz0ctqyanUe.gif" src-width="486" src-height="354" align="center"/>

## 预测

模型训练的最终目的是预测。现在，我们使用测试数据集中的数据来检验模型在未知数据上的表现。这里我们将预测结果的前 6 项进行可视化，他们的实际标签在文本第一行，模型预测结果在文本第二行。

```py
def predict_ch3(net, test_iter, n=6):
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```

可视化结果如下：

<img src="/assets/MbeDbjlxCoJOyixu8k3cVBvon2c.png" src-width="1017" src-height="223" align="center"/>

## 完整代码

```py
import torch
from IPython import display
from d2l import torch as d2l

def softmax(o):
    o_exp = torch.exp(o)
    partition = o_exp.sum(1, keepdim=True)
    return o_exp / partition

def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)

def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

def accuracy(y_hat, y):
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())

class Accumulator:
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()
    metric = Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

class Animator:
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)

def train_epoch_ch3(net, train_iter, loss, updater):
    if isinstance(net, torch.nn.Module):
        net.train()
    metric = Accumulator(3)
    for X, y in train_iter:
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    return metric[0] / metric[2], metric[1] / metric[2]

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc

lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)

num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)

def predict_ch3(net, test_iter, n=6):
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```

# softmax 回归的简洁实现

和线性回归一样，通过调用深度学习框架的 api 也能简化 softmax 回归的实现。这里我们将首先简单讲述 pytorch 中的交叉熵损失函数，再附上第 6 节中代码的简洁版本。

## 重新审视 softmax 的实现

在第 6 节中，我们将未规范化的预测输入 softmax 函数中进行计算，再将结果提供给交叉熵损失函数。这在数学上看起来很合理。然而，从计算角度来看，指数可能会造成数值稳定性问题。考虑到 softmax 函数的形式 $\hat{y_j} = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$ ，如果 $o_k$ 中的一些数值较大，那么经过指数计算后可能会超出数据类型所能容许的上限，即上溢。解决这个问题的一个办法是，在输入 softmax 函数之前，先从所有的 $o_k$ 中减掉 $\max{o_k}$ 。根据如下推导可知，这不改变 softmax 函数的结果。

$$
\begin{align} \hat{y_j} &= \frac{\exp(o_j)}{\sum_k \exp(o_k)} \\  &= \frac{\exp(o_j - \max{o_k}) \cdot \exp(\max{o_k})}{\sum_k \exp(o_k - \max{o_k}) \cdot \exp(\max{o_k})} \\  &= \frac{\exp(o_j - \max{o_k})}{\sum_k \exp(o_k - \max{o_k})} \end{align}$$

这一操作又可能引入新的问题。 $o_j - \max{o_k}$ 可能会具有较大的负值，在经过指数运算和除法运算后，由于精度问题，可能会使 $\hat{y_j}$ 为 0 。这将导致在交叉熵损失函数中的 $\log{\hat{y_j}}$ 的值为 $-\inf$ 。为了避免这个指数运算带来的数值稳定性问题，我们可以考虑将 softmax 和交叉熵结合在一起，将 softmax 中的 $\exp$ 运算与交叉熵中的 $\log$ 运算抵消，即：

$$
\begin{align} \log{\hat{y_j}} &= \log{\frac{\exp(o_j - \max{o_k})}{\sum_k \exp(o_k - \max{o_k})}} \\ &= \log{\exp(o_j - \max{o_k})} - \log{\sum_k \exp(o_k - \max{o_k})} \\ &= o_j - \max{o_k} - \log{\sum_k \exp(o_k - \max{o_k})} \end{align}$$

pytorch 中的交叉熵损失函数实现的就是这个过程。我们可以简单的通过如下代码来调用：

```py
loss = nn.CrossEntropyLoss(reduction='none')
```

