---
title: 模型拟合和优化
slug: ruan-jian-ke-yan-ji-neng-shu/ren-gong-zhi-neng-ai/ji-suan-ji-shi-jue-cv/mo-xing-ni-he-he-you-hua/mo-xing-ni-he-he-you-hua
sidebar_position: 3
---

# 模型拟合和优化

Author：夏彦

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>✅</div>
<p><b>强烈使用这里的：更好的阅读体验</b>：<a href="https://note.isshikih.top/cour_note/D2QD_Intro2CV/Lec04/">https://note.isshikih.top/cour_note/D2QD_Intro2CV/Lec04/</a></p>
<p>公式实在是太难复制了，所以基本上都是挂的，如果有需求可以去原文看，每个小标题的锚点都在，而且文章的源代码也可以在repo里找到</p>
</div>

## 优化

首先我们来定义一个 <b>优化(Optimization)</b> 问题的模型：

设 <b>目标函数</b> $f_0(\vec x)$ 满足 <b>约束条件</b>：

 $\left\{ \begin{array}{ll} &f_i(\vec x) \leq 0, &i = 1,...,m & \text{inequality constraint functions} \\&g_i(\vec x) = 0, &i = 1,...,p & \text{equality constraint functions}& \end{array} \right.$

求 $\vec x\in \mathrm{R}^n$ ，使 $f_0(\vec{x})$ 最小（即最优）。

> 很显然，这和我们高中接触的线性规划很像，实际上线性规划就是其中一种优化问题。

而接下来，我们需要尝试将一些复杂问题转化为优化问题，即根据问题，写出目标函数和约束条件，并通过一些方法来得到我们需要的 $\vec x$。

<b>🌰 图像去模糊问题</b>

在这个 🌰 中，我们已知模糊图像 Y 和模糊滤波器(卷积核) F，需要通过优化的方法来求卷积运算之前的清晰图像 X。

进一步来说，就是找到清晰的图像 X，使得它做模糊处理后与已知的模糊图像 \(Y\) 差别尽可能小，于是问题转化为：

- 目标函数为 $\min\limits_{X} || Y - F * X ||^2_2$ 的优化问题。

### 模型拟合

为了研究分析实际问题，我们需要对问题进行一个建模，更具体的来说就是根据实际情况，寻找数据之间的关系，并建立数学模型。

一个数学 <b>模型(model)</b> 描述问题中输入和输出的关系，例如：线性模型(linear model) b=aTxb=aTx 就描述了输入(input) aa 和输出(output) bb 关于模型参数(model parameter) xx 的关系。

而实际的结果很难严格满足数学模型，这是由多方原因导致的，所以我们往往做的是对实际情况进行 <b>模型拟合(model fitting)</b>。

更具体的来说，我们可能已经有一个先验的假设，即数据符合哪种模型，接下来根据数据来分析得到合适的 model parameters，而这个步骤也常常被称为 learning。

一种比较经典的逼近方法（[最小二乘法](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95)）是求使 <b>均方误差(mean square error)MSE</b> 最小的 model parameters：

x̂ =argminx∑i(bi−aTix)2x^=arg⁡minx⁡∑i(bi−aiTx)2

而如果我们假设数据中的噪声是高斯分布的（实际上大部分噪声在基数足够大的情况下都可以看作为高斯分布的），那么可以与统计学的 <b>极大似然估计(maximum likelihood estimation)MLE</b> 相统一.

<b>MSE vs. MLE</b>

[MSE vs MLE for linear regression](https://medium.com/analytics-vidhya/mse-vs-mle-for-linear-regression-f4ce3f6b990e)

具体来说，bi=aTix+n,n∼G(0,σ)bi=aiTx+n,n∼G(0,σ)，而对于给定的 xx，其 <b>似然(likehood)</b> P[(ai,bi)|x]=P[bi−aTix]∝exp−(bi−aTix)22σ2P[(ai,bi)|x]=P[bi−aiTx]∝exp−(bi−aiTx)22σ2，表示在 model parameter 为 xx 的情况下，数据符合 (ai,bi)(ai,bi) 的可能性。

<b>Maximum Likelihood Estimation</b>

If the data points are <b>independent</b>,

P[(a1,b1)(a2,b2)…|x]=∏iP[(ai,bi)|x]=∏iP[bi−aiTx]∝exp(−∑i(bi−aTix)22σ2)=exp(−||Ax−b||222σ2)P[(a1,b1)(a2,b2)…|x]=∏iP[(ai,bi)|x]=∏iP[bi−aiTx]∝exp⁡(−∑i(bi−aiTx)22σ2)=exp⁡(−||Ax−b||222σ2)

That is, maximize the likelihood to find the best xx.

x̂ =argmaxxP[(a1,b1)(a2,b2)…|x]=argmaxxexp(−||Ax−b||222σ2)=argminx||Ax−b||22x^=arg⁡maxx⁡P[(a1,b1)(a2,b2)…|x]=arg⁡maxx⁡exp⁡(−||Ax−b||222σ2)=arg⁡minx⁡||Ax−b||22

<b>MSE = MLE with Gaussian noise assumption</b>

<b>需要补数学知识，完善这部分内容</b>

---

## 数值方法

上一小节介绍了如何对实际问题进行数学建模，接下来需要介绍的是如何求解数学模型。

我们知道，对于一些比较简单的模型，我们可以直接求其 <b>解析解(analytical solution)</b>，比如使用求导等方法。

<b>🌰</b>

以刚才的线性 MSE 为例，x̂ =argminx∑i(bi−aTix)2x^=arg⁡minx⁡∑i(bi−aiTx)2 等效于求解等式 ATAx=ATbATAx=ATb。

然而，实际情况是大部分问题过于复杂，我们没法直接求其解析解，所以我们需要采用一些即采用一些 <b>数值方法(numerical methods)</b>。

---

### 梯度下降

> 由于相关领域的“函数”等基本上都是高维的，所以我们一般使用二维函数图像的方法来形象表示函数，即使用“等高线”的形式来可视化函数。

虽然没法直接求解析解，但是一般函数都具有一些局部性质，例如极值点临域的梯度都指向极值点。模糊地来说，只要我们随着“梯度”去“下降”，就有可能找到极值点，这就是通过 <b>梯度下降(gradient descent)</b> 的方法来解决优化问题。

简单描述梯度下降的过程：

1. 初始化起点坐标 x；
2. 直到 x 收敛到我们满意的程度之前：
3. 计算下降方向 p；
4. 决定下降步长 ⍺；
5. 更新 x = x + ⍺p；

<img src="/assets/Jl0Ib4RFmo1hsrx4BkBcbNIWnJb.png" src-width="482" src-height="529"/>

> Sourece: [https://commons.wikimedia.org/w/index.php?curid=2283984](https://commons.wikimedia.org/w/index.php?curid=2283984)

其中有三件事需要特殊说明：

- 如何确定下降方向 p⃗ p→
- 如何确定下降步长
- 全局最优和局部最优

#### <b>确定下降方向</b>

对于我们以前接触过的函数，即形式相对简单的函数，我们当然可以直接求其梯度得到下降方向。然而实际问题中的函数可能非常复杂，或梯度很难得到。这时候我们就只能退而求其次，求其“近似”梯度方向。换句话来说，我们希望能够找到一个和原函数在局部和该函数很像的拟合函数，并且用这个拟合函数的梯度方向来决定梯度下降的方向。

于是我们想到泰勒展开，它将函数展开为多项式，而多项式的梯度是相对容易得到的。

其中比较常用的是：

- first-order approximation: F(xk+Δx)≈F(xk)+JFΔxF(xk+Δx)≈F(xk)+JFΔx
- second-order approximation: F(xk+Δx)≈F(xk)+JFΔx+12ΔxTHFΔxF(xk+Δx)≈F(xk)+JFΔx+12ΔxTHFΔx

其中 JFJF 是 [雅各比矩阵](https://zh.m.wikipedia.org/wiki/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)，可以理解为多维向量函数的导数；HFHF 是 [海森矩阵](https://zh.m.wikipedia.org/zh-hans/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%99%A3)，可以理解为多位向量函数的二阶导数。

接下来以 first-order approximation 为例继续分析。

观察 F(xk+Δx)≈F(xk)+JFΔxF(xk+Δx)≈F(xk)+JFΔx，发现当 JfΔx&lt;0JfΔx&lt;0 时， F(x0+Δx)F(x0+Δx) 大概率会比 F(x0)F(x0) 小，即“下降”，所以在 first-order approximation 的情况下，一般选择方向 p⃗ =−JTFp→=−JFT。

#### <b>确定下降步长</b>

即使但从下降速率来考虑，步长太长或太小也都有明显的问题：

<img src="/assets/XTElbdnGHoK0kpxGy8IcFqL6nwj.png" src-width="1650" src-height="1234"/>

所以步长的选择对下降速率的十分关键。

为了专注于步长的选择，我们记 ϕ(α)=F(x+αh),where x and h fixed,x≥0ϕ(α)=F(x+αh),where x and h fixed,x≥0。现在我们希望找到一个能让 ϕ(α)ϕ(α) 尽可能小于 ϕ(0)ϕ(0) 的 αα。接下来我们介绍 Backtracking Algorithm：

1. 初始化 αα 为一个比较大的值；
2. 不断减小 αα 直到 ϕ(α)≤ϕ(0)+γϕ′(0)αϕ(α)≤ϕ(0)+γϕ′(0)α； - 其中 γ∈(0,1)γ∈(0,1) 是一个参数；

<img src="/assets/SkSwbIBl8oGXuWxeWKdclx7hnwf.png" src-width="926" src-height="794"/>

---

上面使用 first-order approximation 为例介绍的这套方法就是 <b>最速梯度下降法(steepest descent method)</b>。

<b>最速梯度下降法</b>

p⃗ =−JTFp→=−JFT；

- 优点
    - 容易实现；
    - 在距离目标点较远时表现良好；

- 缺点
    - 在目标点附近收敛比较慢；
    - 浪费了大量算力；

这是因为，最速梯度下降法主要采用的是 first-order approximation，没有体现曲率特征。

因而一种很自然的改进方法就是使用 second-order approximation，即 <b>牛顿法(Newton Method)</b>。

<b>牛顿法</b>

F(xk+Δx)≈F(xk)+JFΔx+12ΔxTHFΔxF(xk+Δx)≈F(xk)+JFΔx+12ΔxTHFΔx

对它求导得到 ∂F∂Δx=JTF+HFΔx=0∂F∂Δx=JFT+HFΔx=0。

所以得到方向为 p⃗ =Δx=−H−1FJTFp→=Δx=−HF−1JFT

- 优点
    - 在目标点附近下降速度相对更快；

- 缺点
    - Hessian matrix 的计算需要相当算力，甚至有时候无法计算；

由此发现，牛顿法的表现会好很多，如果能够避免计算黑塞矩阵，就可以进一步提高牛顿法的效果，于是就出现了 <b>高斯牛顿法(Gauss-Newton method)</b>。

<b>高斯牛顿法</b>

> 对于解决最小二乘法 x̂ =argminx||R(x)||22x^=arg⁡minx⁡||R(x)||22 的问题表现非常好。

对于这类问题，高斯牛顿法使用 JTRJRJRTJR 来近似代替 HFHF，得到下降方向为 p⃗ =−(JTRJR)−1JTR,where JR is the Jacobian of R(x)p→=−(JRTJR)−1JRT,where JR is the Jacobian of R(x)。

- 优点
    - 不需要计算 Hessian matrix，只需要计算 Jacobian matrix；

- 缺点
    - 由于 JTRJRJRTJR 不正定，所以未必可逆，高斯牛顿法成立的前提是其可逆；
    - 当然，可以使用 Levenberg-Marquardt 算法（LM 算法），即将 JTRJRJRTJR 修正为\(JTRJRJRTJR + \lambda I\) 以保证正定；

<b>Levenberg-Marquardt</b>

Wiki: [🔗](https://zh.wikipedia.org/wiki/%E8%8E%B1%E6%96%87%E4%BC%AF%E6%A0%BC-%E9%A9%AC%E5%A4%B8%E7%89%B9%E6%96%B9%E6%B3%95)

Δx=−(JTRJR+λI)−1JTRR(xk)Δx=−(JRTJR+λI)−1JRTR(xk)

- The effect of λλ:
    - λ→∞λ→∞: Gradient descent, and stepsize is small;
    - λ→0λ→0: Gauss-Newton step;

- How to determine λλ:
    - Update in every iteration;
    - When decreases obviously, λ↑λ↑;
    - When doesn't decrease obviously, λ↓λ↓;

简单来说，LM 就是综合了最速梯度下降法和牛顿法的优点，在远离目标点的时候使用最速梯度下降法，保证启动速度快；在靠近目标点的时候使用牛顿法，保证收敛速度足够快；并且保证 JTRJR+λIJRTJR+λI 正定，所以始终能够使牛顿法成立。

<b>带约束的优化问题</b>

对于 <b>带约束的优化问题(constrained optimization)</b>，则需要 <b>根据实际问题</b> 求解。

- 我们可能可以转化问题，使用更简单的表达式去拟合原来的表达式，来求解；
- 其中，（凹）凸函数是一定能找到最优解的，我们称这种问题为凸优化问题(Convex optimization)

> 推荐同名读物：[https://web.stanford.edu/class/ee364a](https://web.stanford.edu/class/ee364a)

---

#### <b>全局最优和局部最优</b>

分多个 batch 来处理，大量撒点，然后比较每一个 batch 的结果，最终取最好的一个即可。

---

## 鲁棒估计

<b>鲁棒估计(robust estimation)</b> 是对从各种概率分布（尤其是非正态分布）中提取的数据具有良好性能的统计。

在拟合模型中，难免出现一些不符合预期的点，而它们会对拟合结果产生或多或小的影响，而如何权衡这些噪声与真正有用的数据点之间的关系，就是鲁棒估计的课题。

---

### 内点 & 外点

首先我们对数据点进行分类：

- 内点(inlier)：符合我们预期的模型拟合的点；
- 外点(outlier)：完全不符合我们预期模型的点，又叫离群；

<img src="/assets/SaTMb8rIHowolyxOF5WcWdt9nLY.png" src-width="1658" src-height="782"/>

<b>外点的影响</b>

由于外点偏离很大，而最小二乘法中存在平方操作，所以最小二乘法拟合受这些外点影响很大：

<img src="/assets/AWCIbQnK2octpbxxVsccXUcBnRh.png" src-width="1788" src-height="808"/>

---

### 不同的损失函数

于是我们考虑，可以更换拟合的损失函数，来减小大偏差点带来的影响，比如使用 L1 loss(即求绝对值)。不过更好的是一种选择叫 huber loss，它在距离远点较远的时候比较接近 L1 loss。

<b>L1 & L2 & Huber</b>

- L1 loss 即直接对偏差取绝对值，其公式为 L1=|f(x)−Y|L1=|f(x)−Y|，其最大的问题就是在拐点处不光滑，即不可导，而且其收敛可能过快；
- L2 loss 即将偏差取平方，即 L2=|f(x)−Y|2L2=|f(x)−Y|2，L2 loss 在拐点处则是光滑可导的，且收敛相对稳定光滑，但是且在某些时候具有比较好的几何意义，但是对于偏差较大的点更敏感；
- Huber loss 又叫 smooth L1 loss，即使用分段函数，在拐点附近使用 L2 loss，在远端使用 L1 loss；

<img src="/assets/KpthbdGCKotemexvrI0cF9SLnxg.png" src-width="916" src-height="652"/>

- 相关文章：[https://zhuanlan.zhihu.com/p/48426076](https://zhuanlan.zhihu.com/p/48426076)

### 随机抽样一致

<b>随机抽样一致(random sample consensus)RANSAC</b> 采用迭代的方式从包含外点的数据中估计数学模型参数，是一个对于存在明显外点的数据非常有效的方法。

<b>RANSAC procedure</b>

1. 首先随机找两个点拟合一条直线，然后检查有多少点符合这条直线，并对其进行 vote；
2. 重复这个步骤，最后选择票数最高的拟合；

由 outlier 拟合出来的直线一般 votes 比较少，因为 outlier 之间很难一致；但是inlier之间容易一致，因而得分往往更高，于是将它们区分开来。这不就是 Voting Tree (逃

---

## 病态问题

如果一个问题的解不唯一，那么这个问题是一个 <b>病态问题(ill-posed problem)</b>。特别的，在线性问题中，一个线性方程（组）的解如果不唯一（不满秩），则同样是一个病态问题。

当然，对于线性方程组，根据线性代数的知识，我们可以增加方程，即增加约束。而这种约束一般来自于对变量的先验约束，比如：

<b>L2 regularization</b>

<b>L2 norm</b>: ||x||2=σix2i||x||2=σixi2;

<b>L2 regularization</b>: minx||Ax−b||22s.t.||x||2≤1minx||Ax−b||22s.t.||x||2≤1;

> 通过让选择的解尽可能接近原点，而让我们没有用的解的维度尽可能接近 0，以减小没用的变量的影响，抑制冗余变量。

<b>L1 regularization</b>

<b>L1 norm</b>: ||x||1=σi|xi|||x||1=σi|xi|;

<b>L1 regularization</b>: minx||Ax−b||22s.t.||x||1≤1

<img src="/assets/VSorbk6H2oL8xqxeYEDc1VTSn6d.png" src-width="500" src-height="686"/>

> L1 可视化中可以发现，坐标轴上比较容易抓住解，此时意味着有些变量(维度)是 0，换句话来说能让解变得“<b>稀疏(sparse)</b>”，即在维度上的分布只比较集中于个别项。

不过，将他们作为约束条件参与求解，不如直接加进去作为一个项，其效果是等价的：

L2 regularization:minx||Ax−b||22+λ||x||22s.t.||x||2≤1|||or L1 regularization:minx||Ax−b||22+λ||x||1s.t.||x||1≤1L2 regularization:|or L1 regularization:minx||Ax−b||22+λ||x||22|minx||Ax−b||22+λ||x||1s.t.||x||2≤1|s.t.||x||1≤1

---

### 过拟合和欠拟合

在这个过程中，也要小心 <b>过拟合(overfitting)</b> 和 <b>欠拟合(underfitting)</b>，它们的含义非常直白：

---

## 插值

<b>插值(interpolation)</b> 其实已经在 [lec 3 的笔记](https://note.isshikih.top/cour_note/D2QD_Intro2CV/Lec03/#%E6%94%BE%E5%A4%A7%E5%9B%BE%E7%89%87%E5%90%91%E4%B8%8A%E9%87%87%E6%A0%B7--%E6%8F%92%E5%80%BC) 里提到过了。

## 图割 & 马可夫随机场

### 图像标签问题

<b>图像标签问题(image labeling problems)</b> 即通过图片信息给每一个像素分配标签，实际上就是一个对图像内容的分类和识别问题。

其中一个比较强的 <b>先验(prior)</b> 是：相邻且相似的像素应当拥有相同的标签。

而图割和马可夫随机场可以建模这种先验。

---

### 图割

<b>图割(Graphcut)</b> 的核心思想是，把一张图片的每一个像素看作一个 graph 中的 vertex，并在像素之间建 edge，并将 weight 定义为两像素之间的相似性或关联性(affinity or similarity)。

<b>measuring affinity</b>

比如，我们可以这样衡量像素的相似性：

- 设 ff 表示颜色；
- 像素差异(dissimilarity)为 s(fi,fj)=∑k(fik,fjk)2‾‾‾‾‾‾‾‾‾‾‾√s(fi,fj)=∑k(fik,fjk)2；
- 则相似性(affinity)权重为：w(i,j)=A(fi,fj)=e−12σ2s(fi,fj)w(i,j)=A(fi,fj)=e−12σ2s(fi,fj)；

再接下来，通过这样的方式将图片建成图后，就可以把问题转化为图割问题，我们将小权的边删去，最终会形成若干连通分量，而这些连通分量那的点则被视为一个“分割”。

具体来说，图割的代价为断的边权之权重和 cut(VA,VB)=∑u∈VA,v∈VBw(u,v)cut(VA,VB)=∑u∈VA,v∈VBw(u,v)，而我们需要找代价尽可能小的，满足我们要求的图割。当然，这个问题也等效于最大流问题；

<b>Problem with min-cut</b>

> Bias to cut small, isolated segments.

由于 min-cut 的这个问题，我们还需要衡量这个子集是否足够稠密，所以我们倾向于使用 normalized-cut。

assoc(VA,V)=σu∈VA,v∈Vw(u,v)NCut(VA,VB)=cut(VA,VB)assoc(VA,V)+cut(VA,VB)assoc(VB,V)assoc(VA,V)=σu∈VA,v∈Vw(u,v)NCut(VA,VB)=cut(VA,VB)assoc(VA,V)+cut(VA,VB)assoc(VB,V)

- NP-Complete
- Approximate solution by eigenvalue decomposition

---

### 马可夫随机场

<b>马可夫随机场(Markov Random Field)MRF</b> 是一种更通用的解决方案，可惜我第一次没听懂，等我听懂了再来补充这里的内容。

