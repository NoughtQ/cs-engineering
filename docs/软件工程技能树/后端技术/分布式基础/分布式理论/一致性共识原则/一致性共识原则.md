---
title: 一致性共识原则
slug: 一致性共识原则
sidebar_position: 0
---


# 一致性共识原则

Author：蔡龙祥

# 一致性共识协议

## Raft

演示demo

### raft产生的原因

当今应用多运行在动态环境中，而在这种动态环境下数据的一致性（分布在不同地理位置上的服务器中储存的业务信息需要保持一致），安全性以及服务器之间的通信都存在问题，如何使系统能够正常处理服务器的上下线，将故障的处理时间压缩到尽可能短（对于用户来说，过长的服务中断时间是不可接受的）便成了一大挑战：因为在这动态系统中，故障处理，协调，服务发现和配置管理都十分困难。

虽然已有像 Paxos(ViewStampedReplica)和Zab这样的算法能够解决上述问题，但是他们不是以方便程序开发者构建系统以及学生能较为轻松地理解它们为目的，因此设计raft算法的出发点就是在强可理解性的基础上依然保证满足强一致性，安全性，分区容错性，最坏情况下可用性会受损。

为了实现上述需求使用到的技术

1. 分解（将领导者选举，日志复制以及安全性系统分离）
2. 状态空间压缩（目的是减少不确定性）
3. 其他为了实现基于共识的完整系统的所有技术

### raft算法是什么，应用于哪里

Raft协议是一种一致性共识协议，应用于要求强一致（线性一致性）的分布式系统中，raft集群的每个服务器由共识模块，日志条目以及状态机组成。raft算法通常是应用于复制状态机的环境中，日志条目记录着客户端发来的请求指令，包含任期号，索引号以及指令内容，raft集群的服务器有三种身份，领导者，追随者，候选者。只有领导者会处理客户端的请求，一旦处理完毕，并且将该条目复制到大多数服务器中，对日志做完持久化处理后才会提交日志，raft集群的大多数机制的实现都建立在quorum机制之上，所以raft集群节点数目一般为奇数。

领导者选举策略：分布式系统采用逻辑时钟作为系统间通信的时间机制，raft算法使用任期（一个新的领导者当选时会变更任期）作为逻辑时钟，集群中若不存在领导者或者发生领导者宕机的情况，跟随者检测不到领导者的存在就会自动触发超时选举机制，收集选票的集群节点成为候选者，可以同时存在多个候选人，但只能有一个候选人继任，所有竞争选票的候选人的任期都相同且大于上一任领导者的任期，一个节点在一个任期内也只能为一位候选人投票，一旦候选人收集到超过集群节点数半数以上的选票，即可继任，并且会立即向所有节点发布上任通知，在理想情况下集群中的其他所有结点都会成为该节点的跟随者

raft集群的抽象表示

![](/assets/KzIQbgbFBolDFZxSTcrcwSBWnac.png)

共识模块的作用：

接收来自客户端的请求，将其复制到服务器的日志中，并且与其他服务器的共识模块通信，确保一个服务器宕机不会影响整个系统的一致性

复制状态机的作用：

      复制状态机从属于集群的服务器中，通常由三到五台服务器组成，用于计算相同状态的相同副本，用于解决集群分区的容错问题。复制状态机通常的实现方式是复制日志，日志是存储在服务器中的指令集合，正确复制到大多数服务器的日志是已提交的，并且所有服务器中的相同索引+相同任期号的组合唯一对应着一条相同的指令，而状态机顺序执行日志中的已提交的指令，因为状态机是确定的，所以最后得到的结果是完全相同的，然后将结果返回给客户端

### 对于复制状态机的管理

此处介绍一种复制状态机的实现形式

raft定义了三种服务器的角色，由领导者负责管理集群中的其他节点，并使用复制状态机记录状态和日志，被管理的节点处于被动状态，一旦领导者宕机，则会开始选举，直到有新的领导者上台

### raft的特点

1. 领导形式更强大，日志系统更简化
2. 较其他共识算法，更易于理解
3. 简化集群成员的更改和操作，一次只能添加或删除一台服务器，但是确保在更改期间至少有一台服务器与大多数服务器重叠
4. 使用复制状态机实现容错
5. 确保在非拜占庭条件(网络延迟，分区，数据包丢失，重排，重传）下的安全性（返回结果绝对正确），并且安全性不受时间限制，raft采用任期充当逻辑时钟

### raft状态以及RPC定义

Raft集群之间各节点使用RPC进行通信，实现raft协议的不同功能需要按照协议的规则实现对应的RPC方法，RPC调用通常由领导者传递请求参数，发起调用请求，然后各个跟随者收到请求后会在内部调用相关进程得到处理结果作为返回给领导者的响应参数

![](/assets/DovOb0LAGo5ORmx5mWucZHnqnTc.png)

<b>领导者选举/禅让机制的相关RPC：</b>

1. VoteRequest RPC（选举请求，超时自动触发或者由禅让的领导者使继任者发起）
2. AppendEntries RPC（请求参数不携带日志，作为心跳检测）

<b>日志复制机制的相关RPC：</b>

![](/assets/OFcbbjub7olmTtx8VuecoZp6nIf.png)

1. AppendRPC（请求参数携带日志以及检测参数）

![](/assets/WT4Db3n2koITaPxrbUccgKvdnqc.png)

<b>集群成员变更RPC：</b>

1. AddServer RPC
2. RemoveServer RPC
3. InstallSnapshot RPC

 

![](/assets/TPk0bhmOJoWg2hxqh5DcBpvdnTd.png)

<b>客户端交互RPC：</b>

1. ClientRequest RPC
2. RegisterClient RPC
3. ClientQuery RPC

<b> </b>

### raft集群的特性

1. 选举安全性：一个任期只会有1个领导者
2. 领导者只附加性：领导者只会追加自己的日志条目，而不会对其进行覆写或删除
3. 日志匹配特性：不同服务器相同索引相同任期号的日志条目中的内容相同，并且他们之前的日志条目也完全相同。该特性由AppendEntries RPC 自动维护，该rpc中包含的nextindex（初始化为领导者的lastLogIndex+1），prevLogIndex,prevLogTerm会自动检测将要附加的目录之前的日志是否匹配，不匹配则检测失败，领导者下次对该跟随者发起的AppendEntries附带的nextIndex会减1，如此往复，直到找到匹配的日志，然后删除之后不相同的日志，附加与领导者一致的日志，这就保证了日志匹配特性
4. 领导者完整性：领导者拥有所有已经提交的日志。必要的设定如下： 
    1. 如果候选人的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的日志条目，在投票请求RPC中，会判断候选人的日志条目是否不比投票者的日志条目落后，若落后则直接拒绝。保证候选者的日志完整性，以此保证领导者的日志完整性
    2. 领导者复制之前的日志条目时，会保留其任期编号

5. 安全性论证（使用反证法）：

假设：在任期T提交了一条日志，而在大于T任期的U任期内不存在该日志

矛盾：
1.  只有在日志成功复制到大多数节点中才算提交，在T中提交的日志必然会出现在大多数节点中，而U的当选需要大多数节点的支持，两个大多数的集合必然存在交集，因此必然存在至少一个节点拥有T提交的日志。
2.  这个节点必须在接受T的日志之后再向U投票，否则就会因为T的任期小于U的任期而拒绝了T的附加日志请求，导致该节点不存在T的日志，产生矛盾
3.  投票者在投票时依然会保存T中的条目，基于安全性和领导者只附加特性
4.  由于voteRequestRPC的限制，投票者只会向不落后于它的候选者投票，因此U中必然存在T中已经提交的日志条目  

1. 复制状态机安全性：保证不会在同一个日志索引上添加不同的日志内容

### 领导权禅让

为什么要禅让？

当前领导者需要停机维护，或者要退出集群

有优于当前领导者的服务器可以选择

禅让步骤：

1. 当前领导者停止接受客户端请求
2. 向要禅让的客户端发送AppendEntriesRPC来更新目标服务器的日志条目
3. 最后发送一个timeOutNow请求让目标服务器无需等待超时便开启选举，目标服务器很大概率能成为新的领导者

### raft集群成员变更

#### RPC策略以及可行性证明

采用addServer/removeServer RPC更改集群中的单个服务器，禁止会导致多数成员不相交的成员更改，利用已有的日志复制机制进行通信，并且对客户请求以及配置变更请求进行排序使得集群在配置变更期仍然可以处理服务器请求（在logCabin上的实现更为复杂）

一旦新配置的条目复制到了一台服务器上，即告生效，而不是等到新配置提交了才生效

大部分新配置的条目的生效可以确定新配置的提交，一旦提交，则会有如下事实：

1. 大多数集群已应用了新配置，没有收到新配置的服务器不能再构成集群的大多数，也不能成为集群的领导者了
2. 配置更改成功，如果是removeServerRPC，则可以进一步关闭服务器
3. 可以启动下一轮的配置更改了

原文翻译：

在 Raft 中，用于达成一致意见的是调用者的配置，包括投票和日志复制:

- 即使领导者并不在服务器的最新配置中，服务器依然接受它的 AppendEntries 请求。否则将永远不能将新服务器添加到集群中（它永远不会接受添加服务器的配置条目之前的任何日志条目）。
- 服务器还允许投票给不属于服务器当前最新配置的候选人（如果候选人有足够的最新日志和当前任期）。为了保持集群可用，可能偶尔需要进行投票表决。例如，考虑将第四个服务器添加到三个服务器的集群中。如果一个服务器出现故障，就需要新服务器的投票来形成多数票并选出一个领导者。因此，服务器可以直接处理传入的 RPC 请求，而无需查询其当前配置。

#### 添加服务器的维护事项

1. 新添加的服务器是没有日志的，因此需要追赶集群，在这段时间它不具有投票权，不会构成集群的大多数。集群为了了解新加入节点的追赶进度采用了一种机制：
2. 复制操作分为多轮，每轮将当前领导者的所有日志条目复制到新节点的日志中，这样做可以使在本轮中新添加到领导者的日志可以在下一轮复制给新节点，执行固定轮数后，如果最后一轮的持续时间少于一个选举超时时间，则可以判定新节点与领导者的日志差异不会造成显著的可用性缺口，此时新节点正式成为集群成员(有时执行轮数达到了固定轮数也可能会继续追赶)

### 日志压缩

### raft的客户端交互

#### 寻找集群

客户端必须找到raft网络服务所在的位置才能与之交互，一般有两种方法：
1. 客户端使用网络广播或多播方式查找集群成员，类似DHCP方法
2. 通过外部目录服务发现集群成员（DNS），在修改集群成员配置信息之前需要先变更外部目录信息（DNS域名解析记录）

#### 集群内部路由：

raft集群中只有领导者负责处理客户端请求，客户端随机向集群中的节点发起请求，对于发送到集群的客户端请求，需要在集群内部路由到领导者节点。由于AppendEntriesRPC请求中附带了当前任期下的领导者信息，所以大多数服务器都知道领导者的具体信息，当非领导者接收到客户端的请求时，可以选择直接拒绝请求，并返回集群领导者的具体位置，也可以反向代理，直接将请求转发给领导者。

#### 避免无限期延迟请求的策略

1. 领导者：由于网络分区，可能会出现当前的领导者虽然可以和客户端通信，但是无法将日志条目复制到大部分跟随者的情况，而无法提交日志也就无法处理客户端的请求，此时通过心跳检测（发送不带日志条目的AppendEntriesRPC）机制，会进行新的选举，直到集群中存在大多数节点的领导者，这样客户端可以通过另一台服务器重试其请求
2. 跟随者：跟随者必须确保记录的领导者信息没有过时，在每次选举前必须清除记录的领导者信息
3. 客户端：对于失去连接的领导者，客户端选择随机选择一位集群中的其他节点进行连接，而不是重复连接上一个已知的领导者

#### raft的线性化语义

##### raft的线性化要求

所有事务必须按照预期生效，产生预期的效果，状态机执行原子性操作的前后都处于一致性状态。

##### 实现方式

要建立每个客户端请求的一致性语义，raft状态机为此需要对每一个客户端维护了一个会话，在每个客户端启动时会发起Register RPC在集群中注册自己，集群会为该客户端分配一个会话，返回一个标志符信息，以后客户端的每次请求都必须包含该标志符。

会话对于客户端发送的每个独特的请求产生一个唯一的序列号，保存序列号和响应信息的键值，如果接收到小于等于状态机记录的最新已响应序列号的请求，则会直接响应而不执行请求，这实现了对重复请求的过滤。

为了符合上述要求，客户端每次请求要包含其未收到响应信息的最大的序列号信息，而状态机会丢弃所有低于该序列号的记录，节约空间。

##### 会话的过期策略：

1. 设置会话上线数量，采用LRU策略删除设置近期最少使用的条目
2. 集群选择一个意见一致的时间源，每个日志附带当前时间的属性，集群采用确定机制，过期不活跃的会话，客户端需要发出keep-alive请求维持会话的活跃状态

#### 对客户端只读指令的优化

1. 如果领导者尚未将其当前任期的条目标记为已提交，则它将等待直到它已经完成为止。领导者完整性属性可以保证领导者拥有所有已提交的条目，但是在任期开始之初，它可能不知道这些是谁。为了找出答案，它需要从其任期中提交一个条目。Raft 通过让每位领导者在任期开始时在日志中输入一个空白的禁止操作条目来解决此问题。提交此空操作条目后，领导者的 commitIndex 将在其任期内至少与其他任何服务器一样大。
2. 领导者将其当前 commitIndex 保存在局部变量 readIndex 中。这将用作查询所针对的状态版本的下限。
3. 领导者需要确保自己没有被不知道的新领导者所取代。它发出新一轮的心跳，并等待大多数集群成员的确认。收到这些确认后，一旦收到这些确认，领导者知道在发送心跳的那一刻不可能有一个比起任期大的领导者。因此，当时的 readIndex 是集群中任何服务器看到的最大提交索引。
4. 领导者等待其状态机至少应用到 readIndex；这足够满足线性化要求。
5. 最后，领导者针对其状态机发出查询，并向客户端回复结果。

### raft在实现上为了易于理解所做的取舍

不优化其实没什么影响，不要过早优化

1. 通过等待选举超时被动地识别选票瓜分现象
2. 领导者仅能提交当前任期的日志条目
3. 成员变更操作一次只能从集群中删除或增加一个服务器

# Raft实现案例（以go语言为主）

### 6.824 Lab2

基于6.824的LAB在模拟环境中实现了Raft的领导者选举，日志附加以及持久化存储的机制

模拟环境:raft集群运行在由代码构筑的网络之中，可以模拟网络分区以及节点宕机的情况，请求由模拟的客户端发送，由领导者负责接收客户端的请求，并发送给集群中的跟随者

实验目标：实现具有分区容错性和线性一致性的基于raft协议的分布式存储系统

## <b>领导者选举机制（对应LAB 2A 测试）</b>

实现方案：

1. 定义raft节点属性

依附于集群的属性：

保证分布式系统的数据一致性的同步锁：mutex

集群中其他节点的信息：peers

节点在集群中的唯一编号信息:me

用于选举的属性：

当前任期号：term

当前任期投票归属:votedFor

1. 集群中的超时时间阈值设计：

领导者对跟随者的心跳检测间隔时间：150ms

候选者选举超时时间：300ms ± rand（50）ms 

引入随机超时时间，避免多个候选人一直在同一时间开始竞选导致的选票瓜分

1. 对集群中成员的属性的定义：Leader，Candidate,Follower
2. RequestVote RPC实现

 

![](/assets/IIA9b1G4uoWaFjxGlspc6l5Vn1f.png)

请求参数定义

 

![](/assets/T2UcbyHhroXOOrxJvLBcrIVDneb.png)

返回参数定义

 

![](/assets/UFpSbnJkWonwlEx0ZRHce2N2nDb.png)

根据论文中的描述初步确定限制规则：

1. 如何开启选举？
    1. 集群建立之初不存在领导者，每个成员的身份都是跟随者，当经过一个选举超时时间之后会有跟随者转变为候选人开始选举，同时可以存在多个候选人，理论上越早开始选举的候选人当选的概率越大，不过实际情况下由于网络传输导致的丢包问题以及传输时间差异不能忽视，因此不能精确地预测出当选者。
    2. 候选人需要同时向集群中每个节点发送投票请求，为了不阻塞程序的运行，发送请求以及收集选票信息的过程在协程中<b>并发</b>地进行

 

1. 授予选票的判定机制:
    1. 候选人的任期号大于等于<b>投票人</b>的任期
    2. 候选人的日志信息不能落后于<b>投票人</b>
    3. 该任期内尚未给其他<b>候选人</b>投票
    4. 当前任期无<b>候选人</b>当选，且候选人自己只会投票给自己

2. 如何处理请求？
    1. 检测到<b>候选人</b>的任期落后，拒绝投票，并且告知候选人，<b>候选人</b>停止选举，转为<b>跟随者</b>
    2. 根据请求中携带的LastLogIndex属性和LastLogTerm属性校验<b>候选人</b>的日志是否落后，若无落后则授予选票

3. 候选人如何处理响应？
    1. 统计返回的选票信息，如果集群中超过一半的节点支持，停止选举，成为<b>领导者</b>，同时向所有其他节点发起心跳检测，表明自己的身份
    2. 响应中携带了更新的任期号，退出选举，转为<b>跟随者</b>
    3. 对于未返回响应信息的节点，适当重新发送投票请求

 

1. AppendEntries RPC

![](/assets/Ph2Tb2Sl5oamOgx4cBQcHMlKnAd.png)

日志附加与心跳检测共用同一个接口，区别在于心跳检测不会传递日志单元

只需要使用到term 和leaderId 

心跳检测只能由领导者发送，候选人当选为领导者后立即发心跳检测更新集群中节点信息，领导者会每隔一个固定的周期向集群中的所有节点发送心跳检测，当接收者收到请求后，只要检测到任期不大于发送者，就会更新自己的状态为跟随者并且返回成功

## <b>日志附加机制（对应LAB 2B 2C 测试）</b>

对日志属性的定义

 

![](/assets/ExRFbX4ZjoNOejxC8NAcrhFfntb.png)

![](/assets/XkVibO9QDoOIbyxGSkvcMDgHnPf.png)

![](/assets/Rt1ObcQiBoXEHtxhAfBcGnZSnqI.png)

1. AppendEntries RPC 调整：（2B）

心跳检测和日志附加机制共用同一接口，为了减少网络传输中的信息量，日志附加请求只会在匹配成功后才发送日志信息，因此，领导者会维护两个队列，nextIndex保存发送给跟随者的下一条日志的元信息（索引以及任期号），matchIndex保存了对应节点已经添加的日志的最大索引，初始发送的请求参数由这两个队列提供，之后由于日志不匹配的参数调整要依据额外的返回信息，Xindex表明发生冲突的日志的索引位置，Xterm表明发生冲突的日志的任期，Xlen表明冲突日志长度

 

![](/assets/J3JlbWJCYoYzMyxUTCTctYd7nOg.png)

1. 日志提交机制

根据论文中描述，只有领导者会处理客户端的请求，客户端将请求发送给领导者，领导者会将该操作日志存储在自己的日志单元中，并通过日志附加请求向所有跟随者发送该日志，直到领导者收到集群中大多数节点都已经成功复制了日志之后才会将该日志应用并提交，对应LastApplied以及commitIndex

 

![](/assets/AI4XbwXIco0bggxAU0vcCQhenJe.png)

1. 日志应用提交线程(2B)

日志的提交与应用由模拟的状态机实现，实验要求实现的是按照正确的顺序将正确的应该提交的日志传递到状态机的消费队列中，最初的实现思路是在每个RPC调用中都检测日志是否可以提交，但是由于多线程操作，锁的控制粒度较粗，容易导致日志提交乱序，最后采取了为每个raft节点建立一个长期运行的线程负责专门提交日志，只有在commitIndex变更的时候才会触发提交日志的事件，避免了乱序

1. 日志匹配的优化（2B,2C）

采取最朴素的日志匹配方法，每次不匹配就将匹配索引前移一位，这样会导致大量的RPC请求，无法通过时间限制严格的测试点，根据论文中提到的方法，当日志不匹配时需要在跟随者节点中删除不匹配的日志，并且返回冲突的日志任期号以及该任期下第一个节点的索引号，对于跟随者，如果需要匹配的日志索引小于自身日志条目数目，如果失配，就返回跟随者该任期下的第一条日志的索引位置。如果需要匹配的日志条目大于自身日志条目，可以选择直接返回需要匹配的日志索引前一位，或者在日志中查找对应匹配任期的第一个日志索引位置。由于只有再匹配成功后才会发送日志信息，所以最后的性能检测比较好

1. 日志持久化(2C)

分布式系统的运行环境是不可靠的，节点宕机可以发生在任何时候，raft节点宕机恢复需要的信息包括Term，VotedFor（确保不重复投票）以及日志信息，这一块自己需要实现的代码量比较少，只需要在节点关键信息变动的时候及时做持久化处理就可以了

## Etcd

[raft package - github.com/coreos/etcd/raft - Go Packages](https://pkg.go.dev/github.com/coreos/etcd/raft?utm_source=godoc)

## hashicorp/raft

[GitHub - hashicorp/raft: Golang implementation of the Raft consensus protocol](https://github.com/hashicorp/raft)

# Reference

https://web.stanford.edu/~ouster/cgi-bin/papers/raft-atc14

https://github.com/OneSizeFitsQuorum/raft-thesis-zh_cn/blob/master/raft-thesis-zh_cn.md

[Raft Consensus Algorithm](https://raft.github.io/)

https://pdos.csail.mit.edu/6.824/labs/lab-raft.html

