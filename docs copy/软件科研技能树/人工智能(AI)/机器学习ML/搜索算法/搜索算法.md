---
title: 搜索算法
slug: >-
  l3nnwj3ftiporjkfsqacaj4znl4-thmlw28zjigez1kgotycuenonrc-y6gvwxdaniwuhukbkh5cqyk9nud-fupnwsj9bi8ziektuaiczzh2nde-fupnws
sidebar_position: 0
---


# 搜索算法

搜索，即通过一定策略的遍历各种情况，来得到一场游戏的最优决策方式

# 基本概念

## 决策模型

- 状态空间State Space：当前所处的位置
- 行动空间Action Space：当前可以采取的决策行动
- 奖励函数Reward Func：执行当前步骤的价值
- 决策模型：
    - 进行决策：Action = Decide(State, Reward)
    - 实行并观测：State , Reward  = Step(State, Action)

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>💬</div>
<p>Eg. 下棋</p>
<p>状态空间：棋盘上落子情况</p>
<p>行动空间：在不同可落子的地方落子</p>
<p>奖励：当前时刻结束的话，总得分（感觉可以有很多种奖励函数）</p>
</div>

## 搜索模型

状态是节点，行为是有向边

- 树模型：每一层是一个时刻的状态，子节点表示决策后的新状态
    - 存在状态节点重复，不易复用；便于表示随决策进度行动空间会发生变化的情况

- 图模型：表示状态之间所有可能的转移关系
    - 状态节点和行为唯一，方便存储和优化；便于表示多次重复博弈

# 典型搜索算法

## BFS

就不说了，优化之后是A*

## DFS

就不说了，优化之后是Iterative Deepening Search

## A-Star

启发式优化的BFS搜索

### 整体思路

设计一个启发函数H(S)

维护一个优先队列PQ

按照如下顺序遍历节点

```bash
PQ.add(0, S0)
while not PQ.empty():
    S = PQ.popmax()
    do_something_with(S)
    for N in S.neighbors:
        h = H(x) + Cost(S)
        PQ.add(h, N)
```

### 启发函数的设计

所谓的启发式，实际上是利用的问题独特的特性，来指引搜索的大体方向

启发函数具有如下<b>必要性质：</b>

- Admissive：【乐观估计】要求H(x)小于x到终点的最短开销
- Consistent：H(x)与x到终点的最短开销一同减小
- Precise：【乐观估计】要尽量贴近真实最短路径情况
    - 如果H(x)=0, 则退化为BFS
    - 如果H(x)=最短路径，则不可在常数时间实现

在欧氏距离作为启发函数时，给出三条性质的具体表述

- 直线距离比折线距离短
- 直线距离和最短曲线距离同时减小
- 靠近终点时，启发函数近似等于真实距离

## Iterative Deepening Search

基于DFS树搜索优化，每一轮递增深度d，只搜索深度d的节点

## Uniform Cost Search    [UCS]

是dijkstra的增广模型；每个点的历史总权重基于搜索获得，不一定是到起点的最短距离

- 每次维护frontier数组，表示已更新过value的节点
- 每一轮从frontier中遍历节点进行下一步搜索

## MonteCarlo Tree Search    [MCTS]

### 蒙特卡洛方法

- 基于大数定律，使用随机化的采样结果近似代替真实结果的一类算法设计思想

## 算法流程

- Selection 选择<b>最想探索</b>的子节点
    - argmax：各个叶节点的<b>评分rank</b>

- Expansion 生成一个<b>下一步决策</b>
- Simulation 
    - 到此节点的路径为前N次<b>已知</b>决策
    - 从此决策开始，<b>随机</b>向下<b>模拟决策</b>
    - 得到试验<b>得分</b>

- BackPropagation
    - 基于试验<b>得分</b>，刷新搜索数各级父亲的<b>评分rank</b>

- 循环1-4步，直到达到结束条件
    - 结束条件：循环次数、结果精度等

## 节点评分计算

### <b>探索和利用</b>

- 在选择节点时，通常面对的一个问题是<b>探索和利用(Exploration & Exploitation)</b>的冲突
    - 当偏向探索，则搜索收敛速度非常慢
    - 当偏向利用，则可能因为贪心策略陷入局部最优解

- 为了解决此冲突，我们需要一些合理的评分算法，该算法
    - 在搜索早期注重随机化探索
    - 在搜索末期注重最优解邻域进行优化

### 模拟退火

一种显然的想法类似于模拟退火

- 根据搜索进度，在贪心策略中以递减的概率p接受非最优结果
- 直到最后p趋近于0，结果收敛到最优搜索点

### UCB

另外一种常用的算法是<b>Upper Confidence Bound   [UCB]</b>

其数学推导较为复杂，在此简述其<b>核心思想</b>

- 置信度C数据集大小的影响
    - `C = sqrt(2*ln(cnt_visit[parent])/cnt_visit[curr])`

- 令评分为历史搜索平均值在置信度下的上界，即rank = avg(value) + C【那个根号下log的复杂东西】
    <img src="/assets/Wn5pbkJ6mo4VZrx315qcyjzKn0S.png" src-width="1719" src-height="663" align="center"/>

- 则rank的大小即同时表征了探索次数和探索得分，且随时间推移趋向于探索平均分数学期望

- UCT：因为UCB和MCTS结合的算法太过好用和常用，他甚至有了单独的名字——<b>UCT</b>

### 
代码样例

```py
# algo.py
import math
from random import randint, shuffle
from typing import Callable, Iterable

from gym.core import ActType
from treelib import Node, Tree

ValueFunction = Callable[[Iterable[ActType]], float]


class NodeData:
    def __init__(self, act: ActType) -> None:
        self.act = act
        self.is_end: bool = False
        self.cnt_visit: int = 0
        self.cnt_simulate: int = 0
        self.total_reward: float = 0
        self.quality: float = 0
        self.is_end: bool = False
        self.tried_acts: list[ActType] = []


class MCTS:
    def __init__(self, simulate_callback: ValueFunction, actions: list[ActType]) -> None:
        self.tree = Tree()
        self.tree.create_node("ROOT", "ROOT", data=NodeData(None))
        self.simulation_func = simulate_callback
        self.round: int = 0
        self.actions = actions
        self.max_reward = 0

    def run(self, total_rounds: int) -> list[ActType]:
        root_node = self.tree[self.tree.root]
        for self.round in range(total_rounds):
            if self.round % 100 == 0:
                print(f"round {self.round}")
            old_leaf = self.select(root_node)
            if old_leaf is None:
                print("old_leaf is None!")
                continue
            new_leaf = self.expand(old_leaf)
            if new_leaf is None:
                print("new_leaf is None!")
                continue
            reward = self.simulate(new_leaf)
            # print(f"score = {reward}")
            if self.max_reward < reward:
                print(f"max reward: {reward}")
                self.max_reward = reward
            self.back_propagate(new_leaf, reward)

        best_leaf = self.select(root_node)
        return self.acts_to_node(best_leaf)

    def select(self, root: Node) -> Node:
        while True:
            childs = self.tree.children(root.identifier)
            if len(childs)<2:
                return root

            best_score = -100
            best_node: Node = None
            for child in childs:
                score = self.node_score(child)
                if score >= best_score :
                    best_score, best_node = score, child
                    
            if best_node is None:
                root.data.is_end = True

            root = best_node

    def expand(self, node: Node) -> Node:
        if node is None or len(node.data.tried_acts) == len(self.actions):
            return node

        # 随机筛选一个没尝试过的act
        acts = list(
            filter(lambda act: act not in node.data.tried_acts, self.actions))
        shuffle(acts)
        act = acts[0]
        node.data.tried_acts.append(act)
        new_node = self.tree.create_node(data=NodeData(act), parent=node)
        return new_node

    def simulate(self, node: Node) -> float:
        if node.data.is_end:
            return 0

        known_acts = self.acts_to_node(node)

        # s = "".join(["l" if act==0 else "r" for act in known_acts])
        # print(f"acts: {s}")
        # print(f"len(acts)={len(known_acts)}")

        def iter_acts():
            for act in known_acts:
                yield act
            while True:
                yield randint(0, 1)
        score = self.simulation_func(iter_acts())
        return score

    def back_propagate(self, node: Node, reward: float):
        path = self.path_to_node(node)
        if reward < len(path)*0.8:
            node.data.is_end = True
            return

        node.data.cnt_simulate += 1

        for node in path:
            node.data.total_reward += reward
            node.data.cnt_visit += 1


    def path_to_node(self, node: Node) -> list[Node]:
        if node is None: return []
        nodes = [node]
        p = node
        while p := self.tree.parent(p.identifier):
            if p.is_root():
                break
            nodes.append(p)
        nodes.reverse()
        return nodes

    def acts_to_node(self, node: Node) -> list[ActType]:
        return [n.data.act for n in self.path_to_node(node)]

    def node_score(self, node: Node) -> float:
        pnode = self.tree.parent(node.identifier)

        data: NodeData = node.data
        pdata: NodeData = pnode.data if pnode is not None else data
        k = 1.4
        return (data.total_reward/(data.cnt_visit+1)) + k * math.sqrt(math.log2(pdata.cnt_visit+2)/(data.cnt_visit+1))
```

```py
# main.py
from time import sleep
from typing import Iterable

import gym
from gym.core import ActType

from algo import MCTS

env = gym.make("CartPole-v1")

def value_func(actions: Iterable[ActType], delay=.0) -> float:
    total_reward = 0
    env.reset(seed=1)
    for action in actions:
        observation, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        # print(f"action={action}, observation={observation}, reward={reward}")
        if terminated or truncated:
            observation, info = env.reset()
            break
        sleep(delay)
    if total_reward<20: return 0
    return total_reward

mcts = MCTS(value_func, [0,1])
best = mcts.run(1000)

env = gym.make("CartPole-v1", render_mode="human")
env.reset(seed=1)
while True:
    value_func(best, 0.025)
    sleep(1)
```

